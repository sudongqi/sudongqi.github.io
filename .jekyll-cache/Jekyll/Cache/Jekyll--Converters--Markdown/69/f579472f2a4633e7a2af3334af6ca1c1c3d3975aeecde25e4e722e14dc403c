I";<h4 id="abstract"><strong>Abstract</strong></h4>

<p>This post shows how to derive the closed form solution of least square estimation for linear regression. 
After that, we will show how to derive the variance of the coefficient from the closed form solution.</p>

<h4 id="scalar-by-vector-differentiation"><strong>Scalar by Vector Differentiation</strong></h4>

\[\begin{align}
	\frac{d Ax}{dx} &amp;= A \tag{when A is not a function of x} \\
	\frac{dx^T Ax}{dx} &amp;= 2Ax \tag{when A is symmetric} \\
	A &amp;= A^T \tag{Definition of symmetric matrix A} \\
	X^T X &amp;= (X^T X)^T \tag{for any $X \in R^{m \times n}$, $X^T X$ is symmetric}
\end{align}\]

<h4 id="forward-process"><strong>Forward Process</strong></h4>

\[\begin{align}
	X &amp;\in R^{m \times n} \tag{training data with m instances} \\
	y &amp;\in R^{m} \tag{labels for m instances} \\
	\theta &amp;\in R^{n} \tag{regression coefficients as parameters} \\
	X\theta &amp;= \hat{y} \tag{during test time} \\
	J(\theta) &amp;= (X\theta - y)^T(X\theta - y) \tag{objective function: Least Square}
\end{align}\]

<h4 id="get-theta-when-fracd-jthetad-theta--0"><strong>Get \(\theta\) when \(\frac{d J(\theta)}{d \theta} = 0\)</strong></h4>

\[\begin{align}
	J(\theta) &amp;= ((X\theta)^T - y^T)(X\theta - y) \\
	J(\theta) &amp;= (X\theta)^T X\theta - (X\theta)^T y - y^T X\theta + y^T y \\
	J(\theta) &amp;= \theta^T X^T X\theta - 2 (X\theta)^T y + y^T y \tag{$(X\theta)^T y = y^T X\theta$} \\
\end{align}\]

\[\begin{align}
    \frac{d J(\theta)}{d \theta} &amp;= 2 X^T X \theta - 2 X^T y \tag{matrix calculus, $X^T X$ is symmetric} \\
	0 &amp;= 2 X^T X \theta - 2 X^T y \tag{when $\frac{d J(\theta)}{d \theta} = 0$, $J(\theta)$ was minimized} \\
	X^T X \theta &amp;= X^T y  \\
	\theta &amp;= (X^T X)^{-1} X^T y \tag{Least square estimation of $\theta$}
\end{align}\]

<h4 id="variance-of-theta"><strong>Variance of \(\theta\)</strong></h4>
<p>To derive variance of \(\theta\), we need a few useful identities:</p>

\[\begin{align}
    \mathrm{Var}(CX) &amp;= \mathrm{E}\big[ (C (X-\bar{X}))(C (X-\bar{X}))^T \big] \tag{$C$ is a constant matrix} \\
    &amp;= \mathrm{E}\big[ C (X-\bar{X}) (X-\bar{X})^T C^T \big] \\
    &amp;= C \mathrm{E}\big[ (X-\bar{X}) (X-\bar{X})^T \big] C^T \\
    &amp;= C \mathrm{Var}(X) C^T
\end{align}\]

\[\begin{align}
    (X^{-1})^T &amp;= (X^T) ^ {-1} \tag{transpose of inverse is equal to inverse of transpose}
\end{align}\]

<p>With the assumption that all labels are independent with variance \(\sigma^2\), we have:</p>

\[\begin{align}
    \mathrm{Var}(y) &amp;= \sigma^2  I \tag{$I$ is an identity matrix}
\end{align}\]

<p>With above identities, we can calculate the variance of \(\theta\)</p>

\[\begin{align}
    \mathrm{Var}(\theta) &amp;= \mathrm{Var}((X^T X)^{-1} X^T y) \\
    &amp;= (X^T X)^{-1} X^T \sigma^2  I ((X^T X)^{-1} X^T)^T \tag{$(X^T X)^{-1} X^T$ is constant} \\
    &amp;= \sigma^2 (X^T X)^{-1} X^T I X ((X^T X)^T)^{-1} \tag{$\sigma^2$ is constant} \\
    &amp;= \sigma^2 (X^T X)^{-1} X^T X (X^T X)^{-1} \tag{cancel out identity matrix} \\
    &amp;= \sigma^2 (X^T X)^{-1} \tag{$A^{-1}A = I$} \\
\end{align}\]

:ET