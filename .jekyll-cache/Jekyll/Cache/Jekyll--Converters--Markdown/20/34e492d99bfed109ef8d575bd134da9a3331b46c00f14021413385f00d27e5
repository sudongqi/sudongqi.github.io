I"<h4 id="abstract"><strong>Abstract</strong></h4>
<p>In this post we introduce a common view on Expectation Maximization using Jensen’s inequality.</p>

<h4 id="references"><strong>References</strong></h4>

<p><a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf">Andrew’s note</a></p>

<h3 id="em-and-maximum-likelihood-estimation"><strong>EM and maximum likelihood estimation</strong></h3>
<p>The goal of EM is to maximize the likelihood of observed variable X explained by hidden variable Z under a model with 
parameters \(\theta\). Noted that when Z is known (usually as labels), this problem becomes a supervised learning problem and 
it can be solved directly using maximum likelihood estimaation instead of EM. For a training dataset with size \(n\), the expected log likelihood of
a dataset with instances \(x_1\), \(x_2\), … \(x_n\) drawn from distribution \(p_{true}(x)\) is given by:</p>

<p>[\begin{align}
    \mathbb{E}<em>x (\log p</em>{\theta}(x)) \approx \frac{1}{n} \sum_{i=1}^{n} \log p_{\theta}(x_i), \quad x_i \sim p_{true}(x) \tag{0}
\end{align}]</p>

<p>Our objective is to maximize (0) by adjusting \(\theta\)</p>

<p>[\begin{align}
    \operatorname<em>{argmax}_{\theta} \mathbb{E}_x (\log p_{\theta}(x_i)) &amp;= \operatorname</em>{argmax}<em>{\theta} n \cdot \mathbb{E}_x (\log p</em>{\theta}(x_i)) <br />
                                                                        &amp;= \operatorname<em>{argmax}_{\theta} \sum_{i=1}^{n} \log p_{\theta}(x_i) <br />
                                                                        &amp;= \operatorname</em>{argmax}<em>{\theta} \sum</em>{i=1}^{n} \log \sum_{z_i} p_{\theta}(x_i , z_i) <br />
                                                                        &amp;= \operatorname*{argmax}_{\theta} l(\theta) \tag{1}
\end{align}]</p>

<h3 id="derivation-from-jensens-inequality"><strong>Derivation from Jensen’s inequality</strong></h3>
<p>Starting from (1), derivation using Jensen’s inequality adds a “helper” distribution \(q\) such that</p>

<p>[\begin{align}
    l(\theta) &amp;= \sum_{i=1}^{n} \log \sum_{z_i} p_{\theta}(x_i , z_i)  <br />
              &amp;= \sum_{i=1}^{n} \log \sum_{z_i} \frac{q(z_i) p_{\theta}(x_i , z_i)}{q(z_i)} \tag{2} <br />
              &amp; \geq \sum_{i=1}^{n} \sum_{z_i} q(z_i) \log \frac{p_{\theta}(x_i , z_i)}{q(z_i)} \tag{3}
\end{align}]</p>

<p>(2) is greater than or equal to (3) because Jensen’s inequaitliy states that \(f(E[Y]) \geq E[f(Y)]\) 
if \(f\) is a concave function (for more details, look at <a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf">Andrew’s note</a> or <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">wikipedia</a>). 
In our case, \(f\) is \(log\) which is a concave function and \(Y\) is \(\frac{p(x_i , z_i; \theta)}{q(z_i)}\). 
Using (3) as a lower bound of (2), 
we want to raise (3) as close as possible to (2) such that (3) \(=\) (2) to maximize the likelihood. 
For this, Jensen’s inequality also states that when \(Y\) is constant, \(f(E[Y]) = E[f(Y)]\). 
Therefore, we can construct \(q\) such that Y is equal to some constant c as follow</p>

<p>[Y = \frac{p_{\theta}(x_i , z_i)}{q(z_i)} = c \tag{4}]</p>

<p>If we assume each instance has the same corresponding constant c, then we have</p>

<p>[\frac{\sum_{z_i} p_{\theta}(x_i , z_i)}{\sum_{z_i} q(z_i)} = c \tag{5}]</p>

<p>Given \(q(z_i)\) is a probability distribution and \(\sum_{z_i} q(z_i) = 1\), we have</p>

<p>[c = \sum_{z_i} p_{\theta}(x_i , z_i) = p_{\theta}(x_i) \tag{6}]</p>

<p>Therefore, \(q(z_i)\) can be written as follow</p>

<table>
  <tbody>
    <tr>
      <td>[q(z_i) = \frac{p_{\theta}(x_i , z_i)}{p_{\theta}(x_i)} = p_{\theta}(z_i</td>
      <td>x_i) \tag{7}]</td>
    </tr>
  </tbody>
</table>

<h3 id="expectation-maximization"><strong>Expectation Maximization</strong></h3>
<p>(7) is our E step</p>

<table>
  <tbody>
    <tr>
      <td>[q(z_i) = p_{\theta}(z_i</td>
      <td>x_i) \tag{E-step}]</td>
    </tr>
  </tbody>
</table>

<p>Using (7) to replace \(q(z_i)\) in (3), we obtain our M step assuming \(p_{\theta}(x_i \vert z_i)\) is constant</p>

<p>[\begin{align}
    \theta_{new} &amp;= \operatorname<em>{argmax}_{\theta} \sum_{i=1}^{n} \sum_{z_i} p_{\theta}(z_i | x_i) \log \frac{p_{\theta}(x_i , z_i)}{p_{\theta}(z_i | x_i)} <br />
                 &amp;= \operatorname</em>{argmax}<em>{\theta} \sum</em>{i=1}^{n} \sum_{z_i} p_{\theta}(z_i | x_i) \log p_{\theta}(x_i , z_i) - \sum_{z_i} p_{\theta}(z_i | x_i) \log p_{\theta}(z_i | x_i) <br />
                 &amp;= \operatorname<em>{argmax}_{\theta} \sum_{i=1}^{n} \sum_{z_i} p_{\theta}(z_i | x_i) \log p_{\theta}(x_i , z_i) - C <br />
                 &amp;= \operatorname</em>{argmax}<em>{\theta} \sum</em>{i=1}^{n} \sum_{z_i} p_{\theta}(z_i | x_i) \log p_{\theta}(x_i , z_i) \tag{M-step}
\end{align}]</p>

<h4 id="notes"><em><strong>Notes</strong></em></h4>
<p>For (5), it seems this assumption is necessary but ignored from <a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf">Andrew’s note</a> and some other sources. 
How does such assumption affect the construction of \(q(z_i)\)?</p>

:ET