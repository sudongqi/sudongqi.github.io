I"Ú<h4 id="abstract"><strong>Abstract</strong></h4>
<p>In this post we introduce a common view on Expectation Maximization using Jensen‚Äôs inequality.</p>

<h4 id="references"><strong>References</strong></h4>

<p><a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf">Andrew‚Äôs note</a></p>

<h3 id="em-and-maximum-likelihood-estimation"><strong>EM and maximum likelihood estimation</strong></h3>
<p>The goal of EM is to maximize the likelihood of observed variable X explained by hidden variable Z under a model with 
parameters \(\theta\). Noted that when Z is known (usually as labels), this problem becomes a supervised learning problem and 
it can be solved directly using maximum likelihood estimaation instead of EM. For a training dataset with size \(n\), the expected log likelihood of
a dataset with instances \(x_1\), \(x_2\), ‚Ä¶ \(x_n\) drawn from distribution \(p_{true}(x)\) is given by:</p>

\[\begin{align}
    \mathbb{E}_x (\log p_{\theta}(x)) \approx \frac{1}{n} \sum_{i=1}^{n} \log p_{\theta}(x_i), \quad x_i \sim p_{true}(x) \tag{0}
\end{align}\]

<p>Our objective is to maximize (0) by adjusting \(\theta\)</p>

\[\begin{align}
    \operatorname*{argmax}_{\theta} \mathbb{E}_x (\log p_{\theta}(x_i)) &amp;= \operatorname*{argmax}_{\theta} n \cdot \mathbb{E}_x (\log p_{\theta}(x_i)) \\
                                                                        &amp;= \operatorname*{argmax}_{\theta} \sum_{i=1}^{n} \log p_{\theta}(x_i) \\
                                                                        &amp;= \operatorname*{argmax}_{\theta} \sum_{i=1}^{n} \log \sum_{z_i} p_{\theta}(x_i , z_i) \\
                                                                        &amp;= \operatorname*{argmax}_{\theta} l(\theta) \tag{1}
\end{align}\]

<h3 id="derivation-from-jensens-inequality"><strong>Derivation from Jensen‚Äôs inequality</strong></h3>
<p>Starting from (1), derivation using Jensen‚Äôs inequality adds a ‚Äúhelper‚Äù distribution \(q\) such that</p>

\[\begin{align}
    l(\theta) &amp;= \sum_{i=1}^{n} \log \sum_{z_i} p_{\theta}(x_i , z_i)  \\
              &amp;= \sum_{i=1}^{n} \log \sum_{z_i} \frac{q(z_i) p_{\theta}(x_i , z_i)}{q(z_i)} \tag{2} \\
              &amp; \geq \sum_{i=1}^{n} \sum_{z_i} q(z_i) \log \frac{p_{\theta}(x_i , z_i)}{q(z_i)} \tag{3}
\end{align}\]

<p>(2) is greater than or equal to (3) because Jensen‚Äôs inequaitliy states that \(f(E[Y]) \geq E[f(Y)]\) 
if \(f\) is a concave function (for more details, look at <a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf">Andrew‚Äôs note</a> or <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality">wikipedia</a>). 
In our case, \(f\) is \(log\) which is a concave function and \(Y\) is \(\frac{p(x_i , z_i; \theta)}{q(z_i)}\). 
Using (3) as a lower bound of (2), 
we want to raise (3) as close as possible to (2) such that (3) \(=\) (2) to maximize the likelihood. 
For this, Jensen‚Äôs inequality also states that when \(Y\) is constant, \(f(E[Y]) = E[f(Y)]\). 
Therefore, we can construct \(q\) such that Y is equal to some constant c as follow</p>

\[Y = \frac{p_{\theta}(x_i , z_i)}{q(z_i)} = c \tag{4}\]

<p>If we assume each instance has the same corresponding constant c, then we have</p>

\[\frac{\sum_{z_i} p_{\theta}(x_i , z_i)}{\sum_{z_i} q(z_i)} = c \tag{5}\]

<p>Given \(q(z_i)\) is a probability distribution and \(\sum_{z_i} q(z_i) = 1\), we have</p>

\[c = \sum_{z_i} p_{\theta}(x_i , z_i) = p_{\theta}(x_i) \tag{6}\]

<p>Therefore, \(q(z_i)\) can be written as follow</p>

\[q(z_i) = \frac{p_{\theta}(x_i , z_i)}{p_{\theta}(x_i)} = p_{\theta}(z_i | x_i) \tag{7}\]

<h3 id="expectation-maximization"><strong>Expectation Maximization</strong></h3>
<p>(7) is our E step</p>

\[q(z_i) = p_{\theta}(z_i | x_i) \tag{E-step}\]

<p>Using (7) to replace \(q(z_i)\) in (3), we obtain our M step assuming \(p_{\theta}(x_i \vert z_i)\) is constant</p>

\[\begin{align}
    \theta_{new} &amp;= \operatorname*{argmax}_{\theta} \sum_{i=1}^{n} \sum_{z_i} p_{\theta}(z_i | x_i) \log \frac{p_{\theta}(x_i , z_i)}{p_{\theta}(z_i | x_i)} \\
                 &amp;= \operatorname*{argmax}_{\theta} \sum_{i=1}^{n} \sum_{z_i} p_{\theta}(z_i | x_i) \log p_{\theta}(x_i , z_i) - \sum_{z_i} p_{\theta}(z_i | x_i) \log p_{\theta}(z_i | x_i) \\
                 &amp;= \operatorname*{argmax}_{\theta} \sum_{i=1}^{n} \sum_{z_i} p_{\theta}(z_i | x_i) \log p_{\theta}(x_i , z_i) - C \\
                 &amp;= \operatorname*{argmax}_{\theta} \sum_{i=1}^{n} \sum_{z_i} p_{\theta}(z_i | x_i) \log p_{\theta}(x_i , z_i) \tag{M-step}
\end{align}\]

<h4 id="notes"><em><strong>Notes</strong></em></h4>
<p>For (5), it seems this assumption is necessary but ignored from <a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf">Andrew‚Äôs note</a> and some other sources. 
How does such assumption affect the construction of \(q(z_i)\)?</p>

:ET